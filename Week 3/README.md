# **WEEK 3 â€” Transformers + Modern NLP**

### **Concepts**

* 2017 Transformer architecture (Q/K/V, self-attention)
* BERT vs GPT vs encoderâ€“decoder models
* Tokenizers (WordPiece, BPE, SentencePiece)

### **Coding Tasks**

* Fine-tune a BERT model for text classification
* Train a custom SentencePiece tokenizer
* Use Hugging Face `transformers` for inference

### **Mini-Project**

* Build a simple Q&A or text summarizer using a pretrained model


---

## 1ï¸âƒ£ Transformer Architecture (2017)

### What problem did Transformers solve?

Earlier NLP models (RNNs, LSTMs) processed text **sequentially**, which:

* Was slow
* Struggled with long-range dependencies

Transformers replaced recurrence with **self-attention**, allowing **parallel processing** and better context modeling.

---

### Core Components

#### ğŸ”¹ Self-Attention (Q / K / V)

Each token is projected into:

* **Query (Q)** â€“ what am I looking for?
* **Key (K)** â€“ what do I contain?
* **Value (V)** â€“ what information do I give?

Attention score:

```
Attention(Q, K, V) = softmax(QKáµ€ / âˆšdâ‚–) Â· V
```

**Intuition**
Every word asks:

> â€œWhich other words are important for me?â€

Example:

> *â€œThe animal didnâ€™t cross the street because it was tiredâ€*
> â†’ â€œitâ€ attends strongly to â€œanimalâ€

---

#### ğŸ”¹ Multi-Head Attention

Instead of one attention mechanism:

* Use **multiple heads**
* Each head learns a different relationship (syntax, meaning, position)

---

#### ğŸ”¹ Positional Encoding

Transformers donâ€™t have sequence order inherently, so we add **positional encodings**:

* Sinusoidal or learned embeddings
* Inject word order information

---

### ğŸ“š Best Resources

* ğŸ”— **Attention Is All You Need (original paper)**
  [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
* ğŸ¥ Jay Alammar â€” *Illustrated Transformer* (must-read)
  [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
* ğŸ¥ 3Blue1Brown â€” *Attention explained visually*
  [https://www.youtube.com/watch?v=eMlx5fFNoYc](https://www.youtube.com/watch?v=eMlx5fFNoYc)

---

## 2ï¸âƒ£ BERT vs GPT vs Encoderâ€“Decoder

### ğŸ”¹ Encoder-Only â€” **BERT**

* Reads text **bidirectionally**
* Best for **understanding tasks**
* Masked Language Modeling (MLM)

**Used for:**

* Text classification
* NER
* Sentiment analysis
* Semantic search

---

### ğŸ”¹ Decoder-Only â€” **GPT**

* Reads **left-to-right**
* Autoregressive generation

**Used for:**

* Text generation
* Chatbots
* Story writing
* Code generation

---

### ğŸ”¹ Encoderâ€“Decoder â€” T5, BART

* Encoder understands input
* Decoder generates output

**Used for:**

* Translation
* Summarization
* Question answering

---

### ğŸ“Š Quick Comparison

| Model   | Direction     | Strength          |
| ------- | ------------- | ----------------- |
| BERT    | Bidirectional | Understanding     |
| GPT     | Left â†’ Right  | Generation        |
| T5/BART | Both          | Transforming text |

---

### ğŸ“š Resources

* BERT paper: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
* Illustrated BERT: [https://jalammar.github.io/illustrated-bert/](https://jalammar.github.io/illustrated-bert/)
* GPT explained: [https://jalammar.github.io/illustrated-gpt2/](https://jalammar.github.io/illustrated-gpt2/)

---

## 3ï¸âƒ£ Tokenizers (WordPiece, BPE, SentencePiece)

### Why Tokenizers Matter

Neural models donâ€™t see words â€” they see **numbers**.
Tokenizers:

* Break text into subwords
* Handle unknown words
* Control vocabulary size

---

### ğŸ”¹ WordPiece (BERT)

Example:

```
unbelievable â†’ un ##bel ##iev ##able
```

---

### ğŸ”¹ BPE (GPT)

* Merge frequent character pairs
* Efficient and simple

---

### ğŸ”¹ SentencePiece

* Language-agnostic
* Treats text as raw bytes
* No need for whitespace

Used heavily in multilingual models.

---

### ğŸ“š Resources

* SentencePiece paper: [https://arxiv.org/abs/1808.06226](https://arxiv.org/abs/1808.06226)
* Hugging Face tokenizer course:
  [https://huggingface.co/course/chapter2](https://huggingface.co/course/chapter2)

---

## 4ï¸âƒ£ Coding Tasks â€” How to Do Them

---

### âœ… Fine-tune BERT for Text Classification

**Steps**

1. Load pretrained BERT
2. Add classification head
3. Fine-tune on your dataset

**Key Library:** **Hugging Face**

ğŸ“˜ Tutorial:

* [https://huggingface.co/docs/transformers/training](https://huggingface.co/docs/transformers/training)

ğŸ“º Video:

* [https://www.youtube.com/watch?v=8NslS5V5kz0](https://www.youtube.com/watch?v=8NslS5V5kz0)

---

### âœ… Train a Custom SentencePiece Tokenizer

**Why do this?**

* Domain-specific text (medical, legal, Indian languages)

ğŸ“˜ Official guide:

* [https://github.com/google/sentencepiece](https://github.com/google/sentencepiece)

ğŸ“º Practical walkthrough:

* [https://www.youtube.com/watch?v=9d7R8lD4Q2M](https://www.youtube.com/watch?v=9d7R8lD4Q2M)

---

### âœ… Inference Using Transformers

Typical flow:

```python
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
classifier("Transformers are amazing")
```

ğŸ“˜ Docs:

* [https://huggingface.co/docs/transformers/pipeline_tutorial](https://huggingface.co/docs/transformers/pipeline_tutorial)

---

## 5ï¸âƒ£ Mini-Project Ideas (Pick One)

### ğŸ”¹ Q&A System

* Model: `distilbert-base-cased-distilled-squad`
* Input: question + paragraph
* Output: answer span

Tutorial:

* [https://huggingface.co/tasks/question-answering](https://huggingface.co/tasks/question-answering)

---

### ğŸ”¹ Text Summarizer

* Model: `facebook/bart-large-cnn`
* Input: long article
* Output: summary

Tutorial:

* [https://huggingface.co/tasks/summarization](https://huggingface.co/tasks/summarization)

---

### â­ Suggested Deliverable

* CLI or notebook demo
* Explain:

  * Model choice
  * Tokenizer
  * Limitations

---

## ğŸ”š Week 3 Summary

You should finish this week knowing:

* How self-attention actually works
* Why BERT â‰  GPT
* How tokenization impacts models
* How to fine-tune and deploy Transformers

---


