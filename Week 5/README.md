

# ğŸš€ Weeks 5â€“6: Speech-to-Text Model + Deployment

*(Whisper / Wav2Vec 2.0 Track â€” Recommended)*

---

## WEEK 5 â€” Build the Speech-to-Text Model

### ğŸ¯ Goal

Fine-tune a **pretrained STT model** that transcribes short phrases with **reasonable WER**.

---

## 1ï¸âƒ£ Dataset Preparation (Common Voice)

### What you do

* Download **Mozilla Common Voice**
* Filter by language, duration (â‰¤15 s)
* Remove empty / noisy transcripts

### ğŸ“š Resources

**Dataset**

* ğŸ”— [https://commonvoice.mozilla.org](https://commonvoice.mozilla.org)
* HuggingFace loader:

```python
from datasets import load_dataset
dataset = load_dataset("mozilla-foundation/common_voice_13_0", "en")
```

**Videos**

* ğŸ¥ *Fine-tuning Speech Models with HuggingFace* â€” HuggingFace
  [https://www.youtube.com/watch?v=TksaY_FDgnk](https://www.youtube.com/watch?v=TksaY_FDgnk)

**Reading**

* ğŸ“„ Common Voice Paper:
  *â€œCommon Voice: A Massively-Multilingual Speech Corpusâ€*

---

## 2ï¸âƒ£ Audio Preprocessing

### Concepts

* Resampling â†’ **16 kHz**
* Normalization
* Silence trimming (optional)

### Tools

* `torchaudio`
* `librosa`

```python
import torchaudio
waveform, sr = torchaudio.load(path)
waveform = torchaudio.functional.resample(waveform, sr, 16000)
```

### ğŸ“š Resources

* ğŸ“˜ *Speech and Language Processing* â€” Jurafsky & Martin (Ch. 25)
* ğŸ¥ *Audio preprocessing for ML* â€” Valerio Velardo
  [https://www.youtube.com/c/ValerioVelardo](https://www.youtube.com/c/ValerioVelardo)

---

## 3ï¸âƒ£ Train / Test Splits

```python
dataset = dataset["train"].train_test_split(test_size=0.1)
```

### Best practice

* Keep **speaker-independent splits**
* Validate on unseen accents

---

## 4ï¸âƒ£ Load Pretrained STT Model

### Recommended

| Model                  | When to Use                  |
| ---------------------- | ---------------------------- |
| **Whisper-small/base** | Best accuracy, multilingual  |
| **Wav2Vec2-base**      | Lightweight, faster training |

```python
from transformers import WhisperForConditionalGeneration, WhisperProcessor
```

### ğŸ“š Resources

* ğŸ“„ **Whisper Paper** (OpenAI, 2022)
* ğŸ“„ **Wav2Vec 2.0 Paper** (Baevski et al.)
* ğŸ¥ *How Whisper Works* â€” AssemblyAI
  [https://www.youtube.com/watch?v=UeT6U5v8Z3A](https://www.youtube.com/watch?v=UeT6U5v8Z3A)

---

## 5ï¸âƒ£ Tokenize Transcripts

### Whisper

* Built-in tokenizer
* Handles punctuation + casing

### Wav2Vec2

* Character-level tokenizer
* CTC loss

### ğŸ“š Resources

* HuggingFace Docs:

  * [https://huggingface.co/docs/transformers/tasks/asr](https://huggingface.co/docs/transformers/tasks/asr)

---

## 6ï¸âƒ£ Fine-Tune the Model

### Training Setup

* Optimizer: AdamW
* Batch size: 8â€“16
* LR: 1e-5 (Whisper), 3e-4 (Wav2Vec2)

```python
from transformers import Trainer, TrainingArguments
```

### ğŸ“š Resources

* ğŸ¥ *Fine-tuning Whisper End-to-End*
  [https://www.youtube.com/watch?v=ZpZ_7yqF2qE](https://www.youtube.com/watch?v=ZpZ_7yqF2qE)
* ğŸ“˜ *Deep Learning for Audio* â€” MIT 6.S191

---

## 7ï¸âƒ£ Track Metrics (WER)

```python
from evaluate import load
wer = load("wer")
```

### ğŸ“š Resources

* ğŸ“„ *Word Error Rate Explained* â€” NIST
* ğŸ¥ *ASR Evaluation Metrics* â€” CMU

---

## 8ï¸âƒ£ Save & Evaluate Model

```python
trainer.save_model("stt_model")
```

### Milestone

âœ… Transcribes **short phrases**
âœ… WER improves vs baseline

---

# WEEK 6 â€” Final Project + Deployment

## ğŸ¯ Goal

Turn your model into a **real-world STT application**

---

## 1ï¸âƒ£ Model Optimization

### Techniques

* Quantization (INT8)
* Pruning
* ONNX export

```python
torch.quantization.quantize_dynamic(...)
```

### ğŸ“š Resources

* ğŸ“˜ *Efficient Deep Learning* â€” MIT
* ğŸ“„ ONNX Runtime ASR Guide

---

## 2ï¸âƒ£ Real-Time Inference Pipeline

### Flow

```
Mic â†’ Audio Buffer â†’ Feature Extractor â†’ Model â†’ Text
```

### Tools

* `sounddevice`
* `pyaudio`

### ğŸ“š Resources

* ğŸ¥ *Real-time Speech Recognition in Python*
  [https://www.youtube.com/watch?v=JYfHq7j6y6U](https://www.youtube.com/watch?v=JYfHq7j6y6U)

---

## 3ï¸âƒ£ Model Serving

### Recommended Stack

| Tool          | Why              |
| ------------- | ---------------- |
| **Gradio**    | Fast demos       |
| **FastAPI**   | Production-ready |
| **Streamlit** | UI-first         |

```python
import gradio as gr
```

### ğŸ“š Resources

* ğŸ¥ *Deploy Whisper with Gradio*
* ğŸ“˜ *Designing Machine Learning Systems* â€” Chip Huyen

---

## 4ï¸âƒ£ Final Project Deliverables

### âœ” What You Submit

* Fine-tuned Whisper/Wav2Vec2 model
* Web app (upload + mic input)
* Evaluation report:

  * Dataset
  * Architecture
  * Training pipeline
  * WER
  * Demo examples

---

## ğŸ§  Optional Extensions (Advanced)

### Speaker Diarization

* pyannote.audio
  ğŸ“„ *pyannote: Neural Speaker Diarization*

### Punctuation Restoration

* BERT / T5 post-processing

### Mobile Deployment

* ONNX â†’ CoreML / TFLite

---

# ğŸ“š MASTER REFERENCE LIST

### Books

1. **Speech and Language Processing** â€” Jurafsky & Martin
2. **Deep Learning** â€” Goodfellow et al.
3. **Designing ML Systems** â€” Chip Huyen

### Courses

* ğŸ“ Stanford CS224N (Speech lectures)
* ğŸ“ MIT 6.S191 (Audio DL)

### Repos

* [https://github.com/openai/whisper](https://github.com/openai/whisper)
* [https://github.com/facebookresearch/fairseq](https://github.com/facebookresearch/fairseq)
* [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

---

